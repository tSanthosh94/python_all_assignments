{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c32a3eed",
   "metadata": {},
   "source": [
    "# Assignment Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11db1a8f",
   "metadata": {},
   "source": [
    "# Question 1:-What is a parameter?\n",
    "# Ans:-A parameter refers to an internal variable that the model learns during training.\n",
    "\n",
    "# Example:\n",
    "\n",
    "In Linear Regression, parameters are weights (w) and bias (b) in the equation\n",
    "y = w*x + b\n",
    "\n",
    "The model adjusts these parameters to minimize error ‚Äî that‚Äôs what ‚Äútraining‚Äù does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb105b7f",
   "metadata": {},
   "source": [
    "# Question 2.1:-What is correlation?\n",
    "# ans:- Correlation means the relationship between two variables ‚Äî it tells you how strongly and in what direction they move together\n",
    "\n",
    "# Question 2.2:- What does negative correlation mean?\n",
    "# ans:-A negative correlation means that when one variable increases, the other decreases ‚Äî they move in opposite directions\n",
    "\n",
    "# Real-Life Examples:\n",
    "\n",
    "1. Price vs Demand: When the price of a product increases, demand usually decreases.\n",
    "\n",
    "2. Exercise vs Body Fat: More exercise ‚Üí less body fat.\n",
    "\n",
    "3. Speed vs Travel Time: Higher speed ‚Üí less travel time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32fb42c",
   "metadata": {},
   "source": [
    "# Question:-Define Machine Learning. What are the main components in Machine Learning?\n",
    "# ans:-\n",
    "# Definition\n",
    "\n",
    "Machine Learning (ML) is a branch of Artificial Intelligence (AI) that enables computers to learn automatically from data ‚Äî without being explicitly programmed.\n",
    "\n",
    "#  Example:\n",
    "\n",
    "When you teach a model with pictures of cats and dogs, it learns patterns (like shapes, colors, ears, etc.)\n",
    "Later, when you give a new picture, it can predict whether it‚Äôs a cat or a dog ‚Äî without being directly told the rules.\n",
    "\n",
    "#  Main Components of Machine Learning:\n",
    "\n",
    "# 1. Data \n",
    "The foundation of ML.\n",
    "Can be text, images, numbers, or any real-world information.\n",
    "Example: past sales data, medical records, etc.\n",
    "\n",
    "# 2.Features \n",
    "Measurable properties or characteristics of the data used by the model.\n",
    "Example: For a house price model ‚Üí features could be size, location, number of rooms, etc.\n",
    "\n",
    "# 3. Model \n",
    "The mathematical structure that learns patterns from the data.\n",
    "Example: Linear Regression, Decision Tree, Neural Network, etc.\n",
    "\n",
    "# 4.Algorithm \n",
    "The method or process the model uses to learn from data.\n",
    "Example: Gradient Descent, Random Forest algorithm, etc.\n",
    "\n",
    "# 5.Training \n",
    "The process of feeding data to the model so it can learn the relationships between input and output.\n",
    "\n",
    "# 6. Evaluation / Testing \n",
    "Checking how well the trained model performs on new, unseen data.\n",
    "Example metrics: accuracy, precision, recall, F1-score.\n",
    "\n",
    "# 7.Prediction üîÆ\n",
    "After training, the model is used to predict or classify new data.\n",
    "\n",
    "# In Short:- Machine Learning = Data + Algorithm ‚Üí Model ‚Üí Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c679714",
   "metadata": {},
   "source": [
    "# Question 4:-How does loss value help in determining whether the model is good or not?\n",
    "# ans:-The loss value (also called error) tells us how far the model‚Äôs predictions are from the actual (true) values.\n",
    "\n",
    "Low loss value\t:-Model predictions are close to actual values ‚Üí Good performance\n",
    " High loss value:-\tModel predictions are far from actual values ‚Üí Poor performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d15e7b",
   "metadata": {},
   "source": [
    "# Question 5:-What are continuous and categorical variables?\n",
    "# ans:-\n",
    "#  1. Continuous Variables\n",
    "\n",
    "These are variables that can take any numerical value within a range ‚Äî including fractions and decimals.\n",
    "\n",
    "#  Meaning:\n",
    "They represent quantities that can be measured (not just counted).\n",
    "\n",
    "# Examples:\n",
    "\n",
    "Height = 172.5 cm\n",
    "\n",
    "Weight = 65.2 kg\n",
    "\n",
    "Temperature = 36.8¬∞C\n",
    "\n",
    "Time = 2.45 seconds\n",
    "\n",
    "Price = ‚Çπ499.99\n",
    "\n",
    "#  Continuous variables can have infinite possible values within a range.\n",
    "\n",
    "# 2. Categorical Variables\n",
    "\n",
    "These are variables that represent categories, labels, or groups, not numbers.\n",
    "\n",
    "# Meaning:\n",
    "They represent qualities or characteristics ‚Äî values belong to a fixed set of categories.\n",
    "\n",
    "# Examples:\n",
    "\n",
    "Gender = Male / Female / Other\n",
    "\n",
    "City = Delhi / Mumbai / Pune\n",
    "\n",
    "Color = Red / Blue / Green\n",
    "\n",
    "Education Level = High School / Graduate / Postgraduate\n",
    "\n",
    "# Categorical variables have a limited number of distinct values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2affef",
   "metadata": {},
   "source": [
    "# Question 6:- How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
    "# ans:-\n",
    "Machine learning models only understand numbers, not words or labels.\n",
    "So before training, we must convert categorical (text) data into numerical form ‚Äî while preserving its meaning.\n",
    "\n",
    "# Common Techniques to Handle Categorical Variables\n",
    "1.  Label Encoding\n",
    "Each category is assigned a unique integer value.\n",
    "\n",
    "# Example:\n",
    "\n",
    "City\tEncoded\n",
    "Delhi\t0\n",
    "Mumbai\t1\n",
    "Pune\t2\n",
    "\n",
    "# Simple & fast ‚Äî works well for ordinal data (where order matters).\n",
    "# But for nominal data (no order), model might assume numerical relationship (like 2 > 1), which is not true.\n",
    "\n",
    "# Python Example:\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['City'] = le.fit_transform(df['City'])\n",
    "\n",
    "2Ô∏è.  One-Hot Encoding\n",
    "\n",
    "Creates separate binary (0/1) columns for each category.\n",
    "\n",
    "# Example:\n",
    "\n",
    "City\tDelhi\tMumbai\tPune\n",
    "Delhi\t1\t0\t0\n",
    "Mumbai\t0\t1\t0\n",
    "Pune\t0\t0\t1\n",
    "\n",
    "# Works best for nominal data (unordered).\n",
    "# Drawback: creates many new columns if there are many categories.\n",
    "\n",
    "# Python Example:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.get_dummies(df, columns=['City'])\n",
    "\n",
    "3Ô∏è Ordinal Encoding\n",
    "\n",
    "Used when categories have a logical order (rank).\n",
    "\n",
    "# Example:\n",
    "\n",
    "Size\tEncoded\n",
    "Small\t1\n",
    "Medium\t2\n",
    "Large\t3\n",
    "\n",
    "# Best for ordered categories (like size, education level, satisfaction rating).\n",
    "\n",
    "# Python Example:\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "oe = OrdinalEncoder()\n",
    "df['Size'] = oe.fit_transform(df[['Size']])\n",
    "\n",
    "4Ô∏è Target / Mean Encoding (Advanced)\n",
    "\n",
    "Replaces each category with the mean of the target variable for that category.\n",
    "\n",
    "## Often used in high-cardinality data (many categories).\n",
    "# Needs careful handling to avoid data leakage.\n",
    "\n",
    "5Ô∏è Frequency or Count Encoding\n",
    "\n",
    "Replaces each category with how often it appears in the dataset.\n",
    "\n",
    "# Example:\n",
    "\n",
    "City\tFrequency\n",
    "Delhi\t100\n",
    "Mumbai\t150\n",
    "Pune\t80\n",
    "\n",
    "# Useful when the number of categories is large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f65d35",
   "metadata": {},
   "source": [
    "# Question 7:-What do you mean by training and testing a dataset?\n",
    "# ans:-When we build a Machine Learning model, we don‚Äôt train it on all data.Instead, we split the data into two (sometimes three) parts:\n",
    "\n",
    "# Training Set\n",
    "\n",
    "Testing Set(And sometimes a third: Validation Set)\n",
    "\n",
    "# 1Ô∏è Training Dataset\n",
    "\n",
    "This is the data used to train (teach) the model.\n",
    "\n",
    "The model learns the patterns and relationships between input (X) and output (Y) from this data.\n",
    "\n",
    "Example: It learns that more study hours ‚Üí higher exam score.\n",
    "\n",
    "# Example in Python:\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "Here, 80% data = training, 20% data = testing.\n",
    "\n",
    "# 2. Testing Dataset\n",
    "\n",
    "After training, we use testing data to check how well the model performs on unseen data.\n",
    "\n",
    "It helps measure how well the model will perform in the real world.\n",
    "\n",
    "You can think of it like:\n",
    "\n",
    "Training data ‚Üí learning phase \n",
    "\n",
    "Testing data ‚Üí exam phase \n",
    "\n",
    "# Why Split Data?\n",
    "\n",
    "Because we want to know if the model can generalize ‚Äî\n",
    "that means it can make accurate predictions on new data it has never seen before.\n",
    "\n",
    "If a model performs very well on training data but poorly on testing data ‚Üí it‚Äôs overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e013ea87",
   "metadata": {},
   "source": [
    "# Question 8:-What is sklearn.preprocessing?\n",
    "# ans:-sklearn.preprocessing is a module in Scikit-learn (sklearn) that provides tools to prepare your data before training a Machine Learning model.\n",
    "\n",
    "# In simple words:\n",
    " It helps clean, scale, encode, and transform your data into a format that ML algorithms can understand and perform better with.\n",
    "\n",
    "#  Why preprocessing is needed\n",
    "\n",
    "# Raw data usually contains:\n",
    "Different scales (e.g., age in years, salary in lakhs)\n",
    "Categorical (text) values\n",
    "Missing values\n",
    "Uneven distributions\n",
    " # Models like Linear Regression, SVM, or Neural Networks work best when data is normalized, encoded, and consistent ‚Äîthat‚Äôs what preprocessing does!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e883d0",
   "metadata": {},
   "source": [
    "# Question 9:-What is a Test set?\n",
    "# ans:-A Test Set is the portion of your dataset that is used to evaluate how well your trained machine learning model performs on unseen data.\n",
    "\n",
    "# How It Works:\n",
    "You split your data into:\n",
    "Training set ‚Üí used to train the model (learn patterns)\n",
    "Test set ‚Üí used to test the model (check accuracy)\n",
    "You do not show the test data to the model during training.\n",
    "This ensures the test set acts like new, real-world data.\n",
    "\n",
    "# Example:\n",
    "Let‚Äôs say you have 1000 data samples.\n",
    "800 samples ‚Üí training set\n",
    "200 samples ‚Üí test set\n",
    "Your model learns on the 800, and then you test its performance (accuracy, loss, etc.) on the 200 it‚Äôs never seen before.\n",
    "\n",
    "# Purpose of Test Set\n",
    " Goal\t                    Description\n",
    " Evaluate model\t        Checks real-world performance\n",
    " Prevent overfitting\t        Ensures model generalizes beyond training data\n",
    " Compare models\t        Helps decide which model performs best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8a12394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "    Hours_Studied\n",
      "5              6\n",
      "0              1\n",
      "7              8\n",
      "2              3\n",
      "9             10\n",
      "4              5\n",
      "3              4\n",
      "6              7 \n",
      " 5    70\n",
      "0    35\n",
      "7    80\n",
      "2    50\n",
      "9    95\n",
      "4    65\n",
      "3    55\n",
      "6    75\n",
      "Name: Marks, dtype: int64\n",
      "\n",
      "Testing set:\n",
      "    Hours_Studied\n",
      "8              9\n",
      "1              2 \n",
      " 8    90\n",
      "1    40\n",
      "Name: Marks, dtype: int64\n",
      "\n",
      "Predicted marks:\n",
      " [88.40517241 42.84482759]\n",
      "\n",
      "Mean Squared Error: 5.318259512485127\n",
      "R^2 Score: 0.9914907847800238\n",
      "\n",
      "Predicted marks for 7.5 hours studied: 78.64224137931035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vaibh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Question 10.1:-How do we split data for model fitting (training and testing) in Python?\n",
    "# ans:- # Why We Split Data\n",
    "# To train the model on one portion of data (training set).\n",
    "# To evaluate its performance on unseen data (test set).\n",
    "# Prevents overfitting (model memorizing training data instead of learning patterns).\n",
    "\n",
    "# #  Typical Split\n",
    "# Dataset\tPurpose\tTypical Ratio\n",
    "# Training Set\tLearn patterns\t70‚Äì80%\n",
    "# Testing Set\tEvaluate model\t20‚Äì30%\n",
    "# Sometimes, a validation set is also used (especially in deep learning):\n",
    "# Training: 60‚Äì70%\n",
    "# Validation: 10‚Äì20%\n",
    "# Testing: 20‚Äì30%\n",
    "\n",
    "# Python Implementation using scikit-learn\n",
    "# Step 1: Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Step 2: Create a sample dataset\n",
    "data = {\n",
    "    'Hours_Studied': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Marks': [35, 40, 50, 55, 65, 70, 75, 80, 90, 95]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 3: Define features (X) and target (y)\n",
    "X = df[['Hours_Studied']]   # Feature(s)\n",
    "y = df['Marks']             # Target\n",
    "\n",
    "# Step 4: Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 5: Create and train the Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)   # Train the model\n",
    "\n",
    "# Step 6: Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 7: Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Training set:\\n\", X_train, \"\\n\", y_train)\n",
    "print(\"\\nTesting set:\\n\", X_test, \"\\n\", y_test)\n",
    "print(\"\\nPredicted marks:\\n\", y_pred)\n",
    "print(\"\\nMean Squared Error:\", mse)\n",
    "print(\"R^2 Score:\", r2)\n",
    "\n",
    "# Step 8: Optional - Predict marks for new data\n",
    "new_hours = [[7.5]]\n",
    "predicted_marks = model.predict(new_hours)\n",
    "print(\"\\nPredicted marks for 7.5 hours studied:\", predicted_marks[0])\n",
    "\n",
    "\n",
    "# # Parameters:\n",
    "# test_size: Fraction of data for testing (e.g., 0.2 = 20%).\n",
    "# train_size: Fraction of data for training (optional).\n",
    "# random_state: Fixes the split so you get the same train/test sets every time.\n",
    "# shuffle: Randomly shuffles data before splitting (default = True).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2880f7e",
   "metadata": {},
   "source": [
    "# Question 10.2:-How do you approach a Machine Learning problem?\n",
    "#ans:-Approaching a machine learning problem involves several systematic steps to ensure effective model development and deployment. Here‚Äôs a structured approach:\n",
    "# Step 1: Understand the Problem\n",
    "\n",
    "Clearly define the goal of the ML task.\n",
    "Example: Predict house prices, classify emails as spam/ham, detect fraud.\n",
    "\n",
    "Identify input data (features) and output data (target).\n",
    "\n",
    "Determine the type of problem:\n",
    "\n",
    "Regression ‚Üí predict continuous values (price, temperature)\n",
    "\n",
    "Classification ‚Üí predict categories (spam/ham, cat/dog)\n",
    "\n",
    "Clustering ‚Üí group similar data (customer segmentation)\n",
    "\n",
    "# Step 2: Collect and Explore Data\n",
    "\n",
    "Gather data from datasets, databases, or APIs.\n",
    "\n",
    "Explore data with descriptive statistics and visualizations.\n",
    "\n",
    "Identify missing values, outliers, and patterns.\n",
    "\n",
    "understand data types: numeric, categorical, text, images, etc.\n",
    "\n",
    "# Step 3: Preprocess Data\n",
    "\n",
    "Handle missing values ‚Üí drop or impute.\n",
    "\n",
    "Encode categorical variables ‚Üí One-Hot or Label Encoding.\n",
    "\n",
    "Scale/normalize features ‚Üí StandardScaler, MinMaxScaler.\n",
    "\n",
    "Feature engineering ‚Üí create new useful features.\n",
    "\n",
    "# Step 4: Split Data\n",
    "\n",
    "Split into training set and testing set (sometimes also a validation set).\n",
    "\n",
    "Typical ratio: 70‚Äì80% train, 20‚Äì30% test.\n",
    "\n",
    "# Step 5: Choose Model / Algorithm\n",
    "\n",
    "Regression ‚Üí Linear Regression, Decision Tree, Random Forest, XGBoost\n",
    "\n",
    "Classification ‚Üí Logistic Regression, SVM, Random Forest, Neural Networks\n",
    "\n",
    "Clustering ‚Üí K-Means, DBSCAN\n",
    "\n",
    "Start simple, then move to complex models if needed.\n",
    "\n",
    "# Step 6: Train the Model\n",
    "\n",
    "Feed training data to the model.\n",
    "\n",
    "The model learns patterns and optimizes its parameters.\n",
    "\n",
    "# Step 7: Evaluate the Model\n",
    "\n",
    "Use the test set to check model performance.\n",
    "\n",
    "Regression metrics ‚Üí MSE, RMSE, R¬≤ score\n",
    "\n",
    "Classification metrics ‚Üí Accuracy, Precision, Recall, F1-score\n",
    "\n",
    "Check for overfitting (good on train, bad on test) or underfitting (bad on both).\n",
    "\n",
    "# Step 8: Tune the Model\n",
    "\n",
    "Hyperparameter tuning ‚Üí GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "Feature selection ‚Üí remove irrelevant features\n",
    "\n",
    "Regularization ‚Üí L1/L2, Dropout (for neural networks)\n",
    "\n",
    "# Step 9: Deploy / Predict\n",
    "\n",
    "Once satisfied, use the model to predict new data.\n",
    "\n",
    "Optionally deploy it in an application or system.\n",
    "\n",
    "# Step 10: Monitor and Update\n",
    "\n",
    "ML models can degrade over time as data changes.\n",
    "\n",
    "Keep monitoring performance and retrain if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf22ff0f",
   "metadata": {},
   "source": [
    "# Question 11:- Why do we have to perform EDA before fitting a model to the data?\n",
    "# ans:- \n",
    "EDA (Exploratory Data Analysis) is the process of examining, visualizing, and understanding your dataset before building any model.\n",
    "\n",
    "Think of it as ‚Äúgetting to know your data‚Äù before making predictions.\n",
    "\n",
    "# Why EDA is Important Before Fitting a Model\n",
    "# 1Ô∏è Detect Missing or Incorrect Data\n",
    "\n",
    "Missing values, nulls, or wrong entries can break a model or give poor predictions.\n",
    "\n",
    "Example: Age = -5, Salary = ‚Äúabc‚Äù\n",
    "\n",
    "Solution: Handle missing/incorrect values before training.\n",
    "\n",
    "# 2 Understand Data Distribution\n",
    "\n",
    "Helps check whether features are normally distributed, skewed, or have outliers.\n",
    "\n",
    "Some algorithms (like Linear Regression or SVM) perform better if data is scaled or normalized.\n",
    "\n",
    "# 3Ô∏è Identify Outliers\n",
    "\n",
    "Outliers can distort model learning.\n",
    "\n",
    "EDA helps detect and treat them (remove or transform).\n",
    "\n",
    "# 4Ô∏è Explore Relationships Between Variables\n",
    "\n",
    "Check correlations between features and target.\n",
    "\n",
    "Helps in feature selection ‚Äî remove irrelevant or redundant variables.\n",
    "\n",
    "# Example: Hours Studied vs Exam Marks ‚Üí strong positive correlation\n",
    "\n",
    "# 5Ô∏è Decide on Feature Engineering\n",
    "\n",
    "EDA helps create new features or transform existing ones for better model performance.\n",
    "\n",
    "# Example: Extracting ‚Äúday of week‚Äù from a timestamp or log-transforming skewed data.\n",
    "\n",
    "# 6Ô∏èDetect Data Imbalance\n",
    "\n",
    "For classification problems, check if classes are imbalanced.\n",
    "\n",
    "# Example: Fraud detection ‚Üí 99% legitimate, 1% fraud\n",
    "\n",
    "Can decide to resample or use class weights before training.\n",
    "\n",
    "# 7Ô∏è Avoid Garbage-In, Garbage-Out\n",
    "\n",
    "If the data is not understood or cleaned, no model ‚Äî however advanced ‚Äî can give accurate predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4331fd",
   "metadata": {},
   "source": [
    "# Question 12:- What is correlation?\n",
    "#ans:- Correlation is a statistical measure that describes how two variables are related.\n",
    "It tells you:\n",
    "\n",
    "Strength ‚Üí how strongly the variables are related\n",
    "\n",
    "Direction ‚Üí whether they move in the same or opposite direction\n",
    "\n",
    "# Correlation Coefficient (r)\n",
    "\n",
    "A number that quantifies correlation\n",
    "\n",
    "Range: -1 to +1\n",
    "# 0.95 ‚Üí strong positive relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0b790a",
   "metadata": {},
   "source": [
    "# Question 13:-What does negative correlation mean?\n",
    "# Ans:-Negative correlation means that when one variable increases, the other decreases ‚Äî they move in opposite directions.\n",
    " \n",
    " # Graphical Representation\n",
    "\n",
    "On a scatter plot, points slope downward \n",
    "\n",
    "Perfect negative correlation = r = -1\n",
    "\n",
    "No correlation = r = 0\n",
    "\n",
    "# Key Points\n",
    "\n",
    "Negative correlation = inverse relationship\n",
    "\n",
    "Correlation coefficient r < 0 indicates negative correlation\n",
    "\n",
    "The closer r is to -1, the stronger the negative correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2273ac",
   "metadata": {},
   "source": [
    "# Question 15:-What is causation? Explain difference between correlation and causation with an example.\n",
    "# Ans:- Causation (or causal relationship) means that one variable directly affects or causes a change in another variable.\n",
    "\n",
    "# In simple words:\n",
    "A ‚Üí B means ‚ÄúA causes B to happen.‚Äù\n",
    "\n",
    "# Example:\n",
    "\n",
    "Smoking ‚Üí Lung Cancer\n",
    "Smoking directly increases the risk of lung cancer.\n",
    "This is causation.\n",
    "\n",
    "# Correlation vs Causation\n",
    "Aspect\t                Correlation\t                                         Causation\n",
    "Meaning\t                Measures how two variables move together\t         One variable directly affects the other\n",
    "Direction\t            Can be positive, negative, or none\t                 Always implies cause ‚Üí effect\n",
    "Requirement\t            No cause-effect needed\t                             Cause-effect relationship must exist\n",
    "Interpretation\t       ‚ÄúThese variables are related‚Äù\t                    \"This variable changes because of that variable‚Äù\n",
    "Example\t               Ice cream sales ‚Üë correlates with                      Smoking ‚Üí Lung cancer\n",
    "                       drowning ‚Üë (both rise in summer)\t\n",
    "\n",
    "# Key Points\n",
    "\n",
    "Correlation ‚â† Causation\n",
    "\n",
    "Just because two variables are correlated, it does not mean one causes the other.\n",
    "\n",
    "Causation is stronger than correlation\n",
    "\n",
    "It proves that changes in one variable lead to changes in another.\n",
    "\n",
    "Correlation can be misleading\n",
    "\n",
    "# Example:\n",
    "\n",
    "Ice cream sales ‚Üë ‚Üî Drowning cases ‚Üë\n",
    "\n",
    "Correlated but no causal link (both happen because of summer).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e6404e",
   "metadata": {},
   "source": [
    "# Questio 16:- What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
    "#ans:- \n",
    "An optimizer is an algorithm used to update the model‚Äôs parameters (weights and biases) to minimize the loss function during training.\n",
    "\n",
    "# In simple words:\n",
    "The optimizer tells the model how to learn better by adjusting weights to reduce errors.\n",
    "\n",
    "# How it Works\n",
    "\n",
    "Model makes predictions ‚Üí compute loss\n",
    "\n",
    "Optimizer calculates gradients (direction to reduce loss)\n",
    "\n",
    "Model updates weights based on gradients\n",
    "\n",
    "Repeat until loss is minimized\n",
    "\n",
    "# Types of Optimizers\n",
    "\n",
    "There are many optimizers. The most common are:\n",
    "\n",
    "1Ô∏è Gradient Descent (GD)\n",
    "\n",
    "Basic optimizer\n",
    "\n",
    "Updates weights in the direction of negative gradient of the loss function.\n",
    "\n",
    "# Formula:\n",
    "\n",
    "w=w‚àíŒ∑‚ãÖ(‚àÇL/‚àÇw)\n",
    "\n",
    "\t‚Äã\n",
    "\n",
    "\n",
    "where \n",
    "Œ∑ = learning rate\n",
    "\n",
    "Variants:\n",
    "\n",
    "Batch Gradient Descent ‚Äì uses all training data for one update\n",
    "\n",
    "Stochastic Gradient Descent (SGD) ‚Äì uses one sample at a time\n",
    "\n",
    "Mini-batch Gradient Descent ‚Äì uses small batches (common in practice)\n",
    "\n",
    "Example:\n",
    "Training a linear regression model with all data in batch gradient descent.\n",
    "\n",
    "2Ô∏è Momentum\n",
    "\n",
    "Helps accelerate SGD in the right direction and avoid oscillations.\n",
    "\n",
    "Keeps track of previous gradients.\n",
    "\n",
    "# Formula:\n",
    "\n",
    "ùë£=ùõΩùë£+(1‚àíùõΩ)‚àáùêø\n",
    "w=w‚àíŒ∑v\n",
    "Œ≤ = momentum factor (e.g., 0.9)\n",
    "\n",
    "Example:\n",
    "Neural networks often use SGD + momentum to converge faster.\n",
    "\n",
    "3Ô∏è RMSProp (Root Mean Square Propagation)\n",
    "\n",
    "Adaptive learning rate optimizer\n",
    "\n",
    "Uses the moving average of squared gradients to scale learning rate.\n",
    "\n",
    "Solves the problem of vanishing or exploding gradients.\n",
    "\n",
    "Example:\n",
    "\n",
    "Useful in RNNs for sequence prediction tasks.\n",
    "\n",
    "4Ô∏è Adam (Adaptive Moment Estimation)\n",
    "\n",
    "Most popular optimizer in deep learning.\n",
    "\n",
    "Combines Momentum + RMSProp ‚Üí adaptive learning + smooth updates.\n",
    "\n",
    "Works well in practice for almost all neural networks.\n",
    "\n",
    "# Steps:\n",
    "\n",
    "Keeps moving average of gradients (momentum)\n",
    "\n",
    "Keeps moving average of squared gradients (RMSProp)\n",
    "\n",
    "Corrects bias ‚Üí updates weights\n",
    "\n",
    "Example:\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "5Ô∏è Others\n",
    "\n",
    "Adagrad: Adaptive learning rate per parameter, good for sparse data\n",
    "\n",
    "Adadelta: Improvement over Adagrad, avoids decreasing learning rate too much\n",
    "\n",
    "Nadam: Adam + Nesterov momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5414e32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value: 59.99999999999999\n"
     ]
    }
   ],
   "source": [
    "# Question 17:- What is sklearn.linear_model ?\n",
    "# ans:- \n",
    "# sklearn.linear_model is a module in Scikit-learn that provides linear models for regression and classification tasks.\n",
    "\n",
    "# # In simple words:\n",
    "# It contains pre-built algorithms to fit models where the output is linearly related to input features.\n",
    "\n",
    "# #  Why use it?\n",
    "\n",
    "# Easy to understand and implement\n",
    "\n",
    "# Fast to train\n",
    "\n",
    "# Provides baseline models for comparison\n",
    "\n",
    "# Can handle both regression and classification problems\n",
    "\n",
    "#  Example: Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Sample data\n",
    "X = [[1], [2], [3], [4], [5]]\n",
    "y = [10, 20, 30, 40, 50]\n",
    "\n",
    "# Create model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train model\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict([[6]])\n",
    "print(\"Predicted value:\", y_pred[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a8f73bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value: 59.99999999999999\n"
     ]
    }
   ],
   "source": [
    "# Q:-What does model.fit() do? What arguments must be given?\n",
    "#ans:-\n",
    "# In Scikit-learn (and most ML libraries), model.fit() is the method used to train a machine learning model.\n",
    "# # How it works\n",
    "\n",
    "# You give the model features (X) and target (y).\n",
    "\n",
    "# The model uses an algorithm (like Linear Regression, Decision Tree, etc.) to find patterns in the data.\n",
    "\n",
    "# The model stores learned parameters internally.\n",
    "\n",
    "# After .fit(), you can use .predict() on new data.\n",
    "\n",
    "# Example: Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Features (X) and target (y)\n",
    "X = [[1], [2], [3], [4], [5]]\n",
    "y = [10, 20, 30, 40, 50]\n",
    "\n",
    "# Create model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train model\n",
    "model.fit(X, y)  # X = features, y = target\n",
    "\n",
    "# Predict new value\n",
    "y_pred = model.predict([[6]])\n",
    "print(\"Predicted value:\", y_pred[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be2ad0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted values: [60. 70.]\n"
     ]
    }
   ],
   "source": [
    "# Q 19:-What does model.predict() do? What arguments must be given?\n",
    "#ans:-\n",
    "# In Scikit-learn (and most ML libraries), model.predict() is the method used to make predictions using a trained machine learning model.\n",
    "\n",
    "# In simple words:\n",
    "# It uses the patterns learned during training (model.fit()) to predict the target variable for new data.\n",
    "\n",
    "# #  How it works\n",
    "\n",
    "# You first train the model using model.fit(X_train, y_train).\n",
    "\n",
    "# Then you provide new input data (X_test or any new samples).\n",
    "\n",
    "# The model returns predicted output (y_pred) based on learned patterns.\n",
    "# Example: Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Training data\n",
    "X_train = [[1], [2], [3], [4], [5]]\n",
    "y_train = [10, 20, 30, 40, 50]\n",
    "\n",
    "# Create model and train\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict new values\n",
    "X_new = [[6], [7]]\n",
    "y_pred = model.predict(X_new)\n",
    "print(\"Predicted values:\", y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7108a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q 20:-What are continuous and categorical variables?\n",
    "# # ans:-\n",
    "# Continuous Variables\n",
    "#Definition: Variables that can take any numeric value within a range.\n",
    "#Characteristics:\n",
    "# Usually measurable quantities\n",
    "# Can have decimal points\n",
    "# Often used in regression problems\n",
    "\n",
    "# Examples:\n",
    "\n",
    "# Variable\tType\n",
    "# Age\t23, 23.5, 24\n",
    "# Height (cm)\t165.2, 170.5\n",
    "# Temperature (¬∞C)\t36.6, 37.8\n",
    "# Salary\t45000, 51250\n",
    "\n",
    "#  These are numeric and continuous, meaning between any two values, there can be infinite possible values.\n",
    "\n",
    "#  Categorical Variables\n",
    "# Definition: Variables that represent distinct categories or groups.\n",
    "# Characteristics:\n",
    "# Usually non-numeric, but can be numeric labels\n",
    "# Can be nominal (no order) or ordinal (ordered)\n",
    "# Often used in classification problems\n",
    "\n",
    "# Examples:\n",
    "\n",
    "# Variable\tType\n",
    "# Gender\tMale, Female\n",
    "# Blood Group\tA, B, AB, O\n",
    "# Education Level\tHigh School < Bachelor < Master < PhD\n",
    "# City\tDelhi, Mumbai, Pune\n",
    "\n",
    "#  Key Difference\n",
    "# Feature\tContinuous\tCategorical\n",
    "# Nature\tNumeric\tCategories/Groups\n",
    "# Values\tInfinite possibilities\tFinite, distinct options\n",
    "# Examples\tHeight, Weight, Age\tGender, City, Blood Group\n",
    "# ML Use\tRegression\tClassification / Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "706622d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max Scaled Data:\n",
      " [[0.         0.        ]\n",
      " [0.33333333 0.33333333]\n",
      " [0.66666667 0.66666667]\n",
      " [1.         1.        ]]\n",
      "\n",
      "Standardized Data:\n",
      " [[-1.34164079 -1.34164079]\n",
      " [-0.4472136  -0.4472136 ]\n",
      " [ 0.4472136   0.4472136 ]\n",
      " [ 1.34164079  1.34164079]]\n"
     ]
    }
   ],
   "source": [
    "# #Q 21:-What is feature scaling? How does it help in Machine Learning?\n",
    "# #ans:-\n",
    "# Feature scaling is the process of normalizing or standardizing features so that they are on a similar scale.\n",
    "\n",
    "# In simple words:\n",
    "# It ensures that no feature dominates the model just because its values are numerically larger.\n",
    "\n",
    "#Why Feature Scaling is Important\n",
    "#Some ML algorithms use distance between points:\n",
    "\n",
    "# Example: K-Nearest Neighbors (KNN), K-Means\n",
    "# Features with larger ranges will dominate distance calculation.\n",
    "# Gradient-based algorithms (like Linear Regression, Neural Networks, SVM) converge faster when features are scaled.\n",
    "# Avoids bias toward features with larger magnitude.\n",
    "\n",
    "#python Implementation\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {'Height': [150, 160, 170, 180], 'Weight': [50, 60, 70, 80]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "print(\"Min-Max Scaled Data:\\n\", scaled_data)\n",
    "\n",
    "# Standardization\n",
    "scaler_std = StandardScaler()\n",
    "std_data = scaler_std.fit_transform(df)\n",
    "print(\"\\nStandardized Data:\\n\", std_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6681eab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test (scaled):\n",
      " [[ 1.29986737]\n",
      " [-1.29986737]]\n",
      "\n",
      "Predicted Marks: [88.40517241 42.84482759]\n",
      "\n",
      "Actual Marks: [90 40]\n",
      "\n",
      "Mean Squared Error: 5.31825951248517\n",
      "R¬≤ Score: 0.9914907847800237\n",
      "\n",
      "Predicted marks for 7.5 hours studied: 78.64224137931035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vaibh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Q 22:-How do we perform scaling in Python?\n",
    "# ans:-\n",
    "# Step 1: Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Step 2: Create dataset\n",
    "data = {\n",
    "    'Hours_Studied': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Marks': [35, 40, 50, 55, 65, 70, 75, 80, 90, 95]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 3: Split data into features (X) and target (y)\n",
    "X = df[['Hours_Studied']]   # Feature\n",
    "y = df['Marks']             # Target\n",
    "\n",
    "# Step 4: Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 5: Feature scaling (Standardization)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler on training data and transform both train and test\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 6: Train Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 7: Make predictions on test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Step 8: Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Step 9: Print results\n",
    "print(\"X_test (scaled):\\n\", X_test_scaled)\n",
    "print(\"\\nPredicted Marks:\", y_pred)\n",
    "print(\"\\nActual Marks:\", y_test.values)\n",
    "print(\"\\nMean Squared Error:\", mse)\n",
    "print(\"R¬≤ Score:\", r2)\n",
    "\n",
    "# Step 10: Predict for new data\n",
    "new_hours = [[7.5]]\n",
    "new_hours_scaled = scaler.transform(new_hours)\n",
    "predicted_marks = model.predict(new_hours_scaled)\n",
    "print(\"\\nPredicted marks for 7.5 hours studied:\", predicted_marks[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d7b80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Data:\n",
      " [[-1.34164079 -1.34164079]\n",
      " [-0.4472136  -0.4472136 ]\n",
      " [ 0.4472136   0.4472136 ]\n",
      " [ 1.34164079  1.34164079]]\n",
      "One-Hot Encoded Data:\n",
      " [[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# # Q 23:- What is sklearn.preprocessing?\n",
    "# # ans:-\n",
    "# sklearn.preprocessing is a module in Scikit-learn that provides tools to preprocess or transform your data before feeding it to a machine learning model.\n",
    "\n",
    "# In simple words:\n",
    "# It helps you prepare your data so models can learn efficiently and perform better.\n",
    "\n",
    "#  Why Preprocessing is Important\n",
    "\n",
    "# Many ML algorithms assume data is scaled or numeric.\n",
    "#Raw data may have different ranges, missing values, or categorical values.\n",
    "\n",
    "# Preprocessing ensures:\n",
    "# Features are on a similar scale (feature scaling)\n",
    "# Categorical variables are converted to numeric (encoding)\n",
    "# Data is cleaned and ready for modeling\n",
    "\n",
    "## Example: Scaling Data \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "data = {'Height': [150, 160, 170, 180], 'Weight': [50, 60, 70, 80]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "print(\"Scaled Data:\\n\", scaled_data)\n",
    "\n",
    "\n",
    "# Example: Encoding Categorical Data\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Sample categorical data\n",
    "data = [['Male'], ['Female'], ['Female'], ['Male']]\n",
    "\n",
    "# Initialize encoder with new argument\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Fit and transform\n",
    "encoded_data = encoder.fit_transform(data)\n",
    "print(\"One-Hot Encoded Data:\\n\", encoded_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3a64c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test (scaled):\n",
      " [[ 1.29986737]\n",
      " [-1.29986737]]\n",
      "\n",
      "Predicted Marks: [88.40517241 42.84482759]\n",
      "Actual Marks: [90 40]\n",
      "\n",
      "Mean Squared Error: 5.31825951248517\n",
      "R¬≤ Score: 0.9914907847800237\n",
      "\n",
      "Predicted marks for 7.5 hours studied: 78.64224137931035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vaibh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Q 24:-How do we split data for model fitting (training and testing) in Python?\n",
    "#ans:-\n",
    "# Step 1: Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Step 2: Create dataset\n",
    "data = {\n",
    "    'Hours_Studied': [1,2,3,4,5,6,7,8,9,10],\n",
    "    'Marks': [35,40,50,55,65,70,75,80,90,95]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 3: Split data into features (X) and target (y)\n",
    "X = df[['Hours_Studied']]  # Feature\n",
    "y = df['Marks']            # Target\n",
    "\n",
    "# Step 4: Split data into training and testing sets (80%-20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 5: Feature scaling (Standardization)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 6: Train Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 7: Make predictions on test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Step 8: Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Step 9: Print results\n",
    "print(\"X_test (scaled):\\n\", X_test_scaled)\n",
    "print(\"\\nPredicted Marks:\", y_pred)\n",
    "print(\"Actual Marks:\", y_test.values)\n",
    "print(\"\\nMean Squared Error:\", mse)\n",
    "print(\"R¬≤ Score:\", r2)\n",
    "\n",
    "# Step 10: Predict for new data\n",
    "new_hours = [[7.5]]\n",
    "new_hours_scaled = scaler.transform(new_hours)\n",
    "predicted_marks = model.predict(new_hours_scaled)\n",
    "print(\"\\nPredicted marks for 7.5 hours studied:\", predicted_marks[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11dd866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q 25:-Explain data encoding?\n",
    "# # ans:-Data encoding is the process of converting categorical (non-numeric) data into numeric format so that machine learning models can understand and process it.\n",
    "# # Types of Categorical Variables\n",
    "\n",
    "# Nominal: Categories with no natural order\n",
    "    # Example: Color = Red, Blue, Green\n",
    "\n",
    "# Ordinal: Categories with a natural order\n",
    "   # Example: Education = High School < Bachelor < Master < PhD\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
